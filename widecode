#!/usr/bin/env python
import optparse
import sys
from functools import reduce

import numpy as np
from timeit import default_timer as timer
from typing import Sequence

from hw3 import models
from collections import namedtuple, defaultdict

widec_hypothesis = namedtuple("widec_hypothesis",
                              "logprob, lm_state, predecessor, phrase, french_bitmap_coverage, fi, fj")


def bitmap(sequence: Sequence[int]) -> int:
    """ Generate a coverage bitmap for a sequence of indexes """
    return reduce(lambda x, y: x | y, map(lambda i: int('1' + '0' * i, 2), sequence), 0)


def bitmap2str(b, n, on='o', off='.') -> str:
    """ Generate a length-n string representation of bitmap b """
    return '' if n == 0 else (on if b & 1 == 1 else off) + bitmap2str(b >> 1, n - 1, on, off)


def extract_english(h: widec_hypothesis) -> str:
    """Extracts English phrase from hypothesis."""
    return "" if h.predecessor is None else "%s%s " % (extract_english(h.predecessor), h.phrase.english)


def extract_tm_logprob(h: widec_hypothesis) -> float:
    """Recursively extracts logprob of the resulting phrase."""
    return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)


'''
Algorithm

1. Find all possible translations from TM for all possible phrases.
 Each translation phrase can be a start, the goal is to cover all input words.
2. Introduce reordering penalty to estimation process.
3. Model beam search with optimizations.

'''

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="hw3/data/input",
                     help="File containing sentences to translate (default=hw3/data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="hw3/data/tm",
                     help="File containing translation model (default=hw3/data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="hw3/data/lm",
                     help="File containing ARPA-format language model (default=hw3/data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxsize, type="int",
                     help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=20, type="int",
                     help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s", "--stack-size", dest="s", default=10000, type="int", help="Maximum stack size (default=1)")
optparser.add_option("-a", "--alpha-threshold", dest="alpha", default=0.9, type="float",
                     help="Value of alpha threshold for stack pruning: "
                          "only prob(h) >= alpha * prob(best_h) should be kept in the stack."
                          "By default, we disable threshold pruning (alpha = 0).")
optparser.add_option("-d", "--reordering-limit", dest="d", default=sys.maxsize,
                     type="int", help="Reordering limit to restrict maximum possible reordering distance.")
optparser.add_option("-b", "--reordering-base", dest="b", default=0.3,
                     type="float", help="Base for the distance-function. Zero (0.0) value disables reordering penalty.")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,
                     help="Verbose mode (default=off)")
opts = optparser.parse_args()[0]

if not (0 <= opts.alpha <= 1):
    raise Exception('Cannot run decoding with alpha (stack threshold ratio) less than 0 or more than 1.')

tm = models.TM(opts.tm, opts.k)
lm = models.LM(opts.lm)
french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

# tm should translate unknown words as-is with probability 1
for word in set(sum(french, ())):
    if (word,) not in tm:
        tm[(word,)] = [models.phrase(word, 0.0)]

initial_hypothesis = widec_hypothesis(0.0, lm.begin(), None, None, 0, None, None)

for i in range(9):
    k = np.log10(opts.b ** abs(i))
    t = 1

start = timer()
for f in french:
    all_phrases_translations = defaultdict(list)
    # 1) Generate all possible french spans
    # 2) Collect all possible translations
    for fi in range(len(f)):
        for fj in range(fi + 1, len(f) + 1):
            english_translations = tm.get(f[fi:fj], [])
            for phrase_obj in english_translations:
                all_phrases_translations[(fi, fj)].append(phrase_obj)

    stacks = [{} for _ in f] + [{}]
    # 0 French words translated - empty hypothesis for English side
    stacks[0][lm.begin()] = initial_hypothesis

    '''
    place empty hypothesis into stack 0 for all stacks 0...n âˆ’ 1 do
    for all hypotheses in stack do 
        for all translation options do
            if applicable then
            create new hypothesis
            place in stack
            recombine with existing hypothesis if possible 
            prune stack if too big
    '''

    # iterates by all stacks except the last one which contains the result
    local_start = timer()
    for i, stack in enumerate(stacks[:-1]):
        # iterate by all hypotheses covering i French words

        # histogram pruning
        pruned_hypotheses = sorted(stack.values(), key=lambda h: -h.logprob)[:opts.s]
        # best logprob is a small negative number
        best_h_prob = pruned_hypotheses[0].logprob
        # threshold value is a positive number
        threshold_value = -1 * (1/opts.alpha) * best_h_prob
        # we keep only hypotheses, which have -logprob less than the threshold, defined above
        pruned_hypotheses = [h for h in pruned_hypotheses if -h.logprob <= threshold_value]
        for h in pruned_hypotheses:  # prune
            # find all possible next spans of length [1, len(f) + 1 - i]
            for span_len in range(1, len(f) + 1 - i):
                for fi in range(len(f)):
                    fj = min(fi + span_len, len(f) + 1)
                    # if selected French words are not translated
                    if bitmap(range(fi, fj)) & h.french_bitmap_coverage == 0:
                        f_words_translated = i + (fj - fi)

                        # prune
                        if len(stacks[f_words_translated]) > opts.s:
                            continue

                        new_french_bitmap_coverage = bitmap(range(fi, fj)) | h.french_bitmap_coverage
                        english_translations = all_phrases_translations.get((fi, fj), [])
                        for phrase in english_translations:
                            # accumulate TM-logprob with prev hypothesis
                            logprob = h.logprob + phrase.logprob
                            lm_state = h.lm_state
                            # add LM-logprob
                            for word in phrase.english.split():
                                (lm_state, word_logprob) = lm.score(lm_state, word)
                                # by summing logprobs we are basically multiplying absolute values
                                logprob += word_logprob
                            logprob += lm.end(lm_state) if f_words_translated == len(f) else 0.0

                            # if reordering base is set (not 0.0)
                            # and h is not an initial hypothesis, since initial one does not have fi
                            if opts.b and h.fi is not None:
                                # calculate reordering distance: start_i - end_(i-1) - 1
                                reordering_distance = fi - h.fj - 1
                                reordering_score = np.log10(opts.b**abs(reordering_distance))
                                # add reordering score
                                logprob += reordering_score

                            new_hypothesis = \
                                widec_hypothesis(logprob, lm_state, h, phrase, new_french_bitmap_coverage, fi, fj)

                            # second case is recombination, keep in mind that logprobs are negative
                            if lm_state not in stacks[f_words_translated] \
                                    or stacks[f_words_translated][lm_state].logprob < logprob:
                                stacks[f_words_translated][lm_state] = new_hypothesis

    winner = max(stacks[-1].values(), key=lambda h: h.logprob)
    predicted_translation = extract_english(winner)
    print(predicted_translation)
    local_end = timer()

    if opts.verbose:
        sys.stderr.write(f'%f seconds elapsed: ' % (local_end - local_start))
        tm_logprob = extract_tm_logprob(winner)
        sys.stderr.write("LM = %f, TM = %f, Total = %f\n" %
                         (winner.logprob - tm_logprob, tm_logprob, winner.logprob))

end = timer()
sys.stdout.write("\nCalculated in : %f seconds\n" % (end-start))
